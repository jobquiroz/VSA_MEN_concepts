{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the knowledge base into Hyperdimensional Vectors\n",
    "\n",
    "In this notebook the functions from the 'HDComputing' notebook are used to encode the McRae dataset. The following functions create an heteroassociative memory in which a knowledge base of Semantic Features representation of concepts is stored.\n",
    "\n",
    "### Importing libraries and HD computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#Only done once... \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('wordnet_ic')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "\n",
    "%run HDComputing_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateFeats(ListFeat):\n",
    "    \"It receives a list of features such as ['is_blue', 'is_rectangular'] and it returns: [['color','blue'], ['shape','rectangular']\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'FEATS_brm.xlsx') #../McRaedataset/FEATS_brm.xlsx')\n",
    "    ListPairs = []\n",
    "    for feat in ListFeat:\n",
    "        # Row for feature...\n",
    "        row = df.loc[df['Feature'] == feat]       \n",
    "        # Look for values in vec_feat and vec_value\n",
    "        ListPairs.append([str(row['feat_name'].tolist()[0]), str(row['feat_value'].tolist()[0])])       \n",
    "    return ListPairs\n",
    "\n",
    "def ClosestConcepts (concept, nc):\n",
    "    \"Given a concept label, this function reads the distance matrix from McRae's and returns the 'nc' closests concepts in a list\"\n",
    "    # Excel document to data frame...\n",
    "    try:\n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200') #../McRaeDataset/cos_matrix_brm_IFR.xlsx', '1st_200')\n",
    "        ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    except: \n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200') # ('../McRaeDataset/cos_matrix_brm_IFR.xlsx', '2nd_200')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141') #('../McRaeDataset/cos_matrix_brm_IFR.xlsx', 'last_141')\n",
    "            ordered = df.sort_values(by=concept, ascending=False)[['CONCEPT', concept]]\n",
    "    \n",
    "    L1 = list(ordered['CONCEPT'][0:nc])\n",
    "    L1 = map(str, L1)\n",
    "    L2 = zip(L1,list(ordered[concept][0:nc]))\n",
    "    L2 = map(list, L2)\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features\n",
    "\n",
    "### Normal encoding\n",
    "All features are included in definition\n",
    "\n",
    "(PONER ECUACIÃ“N....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDefinitions(max_num_feats, names_list): #Con_List is the list of concepts to encode... \n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx') #../McRaeDataset/CONCS_FEATS_concstats_brm.xlsx') #MINI_\n",
    "    #Create a list with all concept names\n",
    "    #names = set(df['Concept'])\n",
    "    \n",
    "    # Extract list of features for each name\n",
    "    Concepts = []\n",
    "    for n in names_list:\n",
    "        row = df.loc[df['Concept'] == n]\n",
    "        Concepts.append([str(n), map(str,list(row['Feature']))[:max_num_feats]])\n",
    "    return Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar function for giving weights to features\n",
    "\n",
    "It repeats features within the feature's list...\n",
    "\n",
    "- If the list lenght (len) is less than or equal to 3 does nothing... [[feat_1 , val_1], [feat_2, val_2], [feat_2, val_2]] -> \" \"\n",
    "\n",
    "- If 3 < len <= 5 repeats first 2 two times... \n",
    "        [[feat_1, val_1],...,[feat_5, val_5]] -> [[feat_1, val_1], [feat_2, val_2], [feat_2, val_2], [feat_3, val_3], \n",
    "                                                 [feat_4, val_4], [feat_5, val_5]]\n",
    "                                                                                       \n",
    "- If 5 < len <= 8, repeats feat 1 and 2 three times, feats 3 and 4 twice and 5 - 8 once... \n",
    "\n",
    "- If 8 < len repeats feat 1 and 2 four times, 3 and 4 three times, 5 - 7 twice, and 8 -> once... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Repeat_features(selected_features):\n",
    "    \"It receives a list of features and returns with repeated elements. Repetition ensembles weight\"\n",
    "    L = []\n",
    "    if len(selected_features) <= 3:\n",
    "        return selected_features\n",
    "    elif len(selected_features) <= 5:\n",
    "        for i in range(len(selected_features)):\n",
    "            if i < 2:\n",
    "                L.extend([selected_features[i]]*2)\n",
    "            else:\n",
    "                L.extend([selected_features[i]])\n",
    "                \n",
    "    elif len(selected_features) <= 8:\n",
    "        for i in range(len(selected_features)):\n",
    "            if i < 2:\n",
    "                L.extend([selected_features[i]]*3)\n",
    "            elif i < 4:\n",
    "                L.extend([selected_features[i]]*2)\n",
    "            else:\n",
    "                L.extend([selected_features[i]])\n",
    "    else:\n",
    "        for i in range(len(selected_features)):\n",
    "            if i < 2:\n",
    "                L.extend([selected_features[i]]*4)\n",
    "            elif i < 4:\n",
    "                L.extend([selected_features[i]]*3)\n",
    "            elif i < 7:\n",
    "                L.extend([selected_features[i]]*2)\n",
    "            else:\n",
    "                L.extend([selected_features[i]])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by *Rank_PF*\n",
    "\n",
    "Only the *max_num_feats* features with the highest frequency are encoded..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDefs_RankPF(max_num_feats, names_list):\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    \n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx')\n",
    "    \n",
    "    #Create a list with all concept names\n",
    "    #names = df['Concept'].unique().tolist()\n",
    "    #names = map(str, names)\n",
    "    Concepts = []\n",
    "    \n",
    "    # Extract list of features for each concept\n",
    "    for name in names_list:\n",
    "        # Locating the concept by name\n",
    "        row = df.loc[df['Concept'] == name]\n",
    "        \n",
    "        # Reading features and Rank_PF values\n",
    "        selected_features = row[['Feature','Rank_PF']].values.tolist()\n",
    "        \n",
    "        # Setting strings into an appropiate format\n",
    "        selected_features = map(lambda x: [str(x[0]), int(x[1])], selected_features)\n",
    "        \n",
    "        # Sorting by Rank_PF and keeping only the most frequent (the max_num_feats most common)... \n",
    "        selected_features = sorted(selected_features, key = lambda x: x[1])[:max_num_feats]\n",
    "        \n",
    "        # Keeping only the feature's name (removing Rank_PF value)\n",
    "        selected_features = [x[0] for x in selected_features]\n",
    "\n",
    "        # Creating final representation \n",
    "        Concepts.append([str(name), Repeat_features(selected_features)]) \n",
    "    return Concepts\n",
    "\n",
    "#Defs = ReadDefs_RankPF(10)\n",
    "#print Defs\n",
    "\n",
    "# Statistical measures\n",
    "#numero_feats = [len(set(x[1])) for x in Defs]\n",
    "#print numero_feats, len(numero_feats), max(numero_feats), min(numero_feats), sum(numero_feats)/len(numero_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by *Disting (D)*\n",
    "\n",
    "Only the features classified as *Disting* in the dataset are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDefs_Disting(max_num_feats, names_list):\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    \n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx')\n",
    "    \n",
    "    #Create a list with all concept names\n",
    "    #names = df['Concept'].unique().tolist()\n",
    "    #names = map(str, names)\n",
    "    \n",
    "    Concepts = []\n",
    "    # Extract list of features for each concept\n",
    "    for name in names_list:\n",
    "        # Locating the concept by name\n",
    "        row = df.loc[df['Concept'] == name]\n",
    "        \n",
    "        # Reading features and Rank_PF values\n",
    "        selected_features = row[['Feature','Disting']].values.tolist()\n",
    "        # Setting strings into an appropiate format\n",
    "        selected_features = map(lambda x: [str(x[0]), str(x[1])], selected_features) \n",
    "        # Select only the 'D' features \n",
    "        selected_features = [x[0] for x in selected_features if x[1] == 'D']\n",
    "\n",
    "        Concepts.append([str(name), Repeat_features(selected_features[:max_num_feats])]) \n",
    "    return Concepts\n",
    "\n",
    "#Defs = ReadDefs_Disting(6)\n",
    "#print Defs\n",
    "\n",
    "# Luego de eliminar los features con valor intercorr = 0 quiero ver cual es el promedio de features por concepto,\n",
    "# asÃ­ como max y min\n",
    "#numero_feats = [len(set(x[1])) for x in Defs]\n",
    "#print numero_feats, len(numero_feats), max(numero_feats), min(numero_feats), sum(numero_feats)/len(numero_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by *Intercorr_str_tax*\n",
    "\n",
    "According to McRae dataset this variable: measures \"*intercorrelational strength of feature for that concept*\"\n",
    "Not combining... pure Intercorr_str_tax and conditions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDefs_Intercorr_str (max_num_feats, names_list):\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx')\n",
    "    \n",
    "    #Create a list with all concept names\n",
    "    #names = df['Concept'].unique().tolist()\n",
    "    #names = map(str, names)\n",
    "    \n",
    "    Concepts = []\n",
    "    # Extract list of features for each concept\n",
    "    for name in names_list:\n",
    "        # Locating the concept by name\n",
    "        row = df.loc[df['Concept'] == name]\n",
    "        \n",
    "        # Reading features and Rank_PF values\n",
    "        selected_features = row[['Feature','Intercorr_Str_Tax']].values.tolist()\n",
    "        # Setting strings into an appropiate format\n",
    "        selected_features = map(lambda x: [str(x[0]), float(x[1])], selected_features) \n",
    "        \n",
    "        # Keeping only the features where Intercorr_Str_Tax is higher than 0... \n",
    "        selected_features = [x for x in selected_features if x[1] > 0]\n",
    "        \n",
    "        # Sorting by Intercorr_Str_Tax and keeping only the highest (the 'max_num_feats' highest)... \n",
    "        selected_features = sorted(selected_features, key = lambda x: x[1])[:max_num_feats]\n",
    "        \n",
    "        # Keeping only the feature's name (removing Intercorr_Str_Tax value)\n",
    "        selected_features = [x[0] for x in selected_features]\n",
    "\n",
    "        # Creating final representation \n",
    "        Concepts.append([str(name), Repeat_features(selected_features)]) \n",
    "    return Concepts\n",
    "\n",
    "#Defs = ReadDefs_Intercorr_str(8)\n",
    "#print Defs\n",
    "# hacer pruebas para ver si alguna no tiene disting... o cual es el mÃ¡x y cual es el min... \n",
    "\n",
    "# Luego de eliminar los features con valor intercorr = 0 quiero ver cual es el promedio de features por concepto,\n",
    "# asÃ­ como max y min\n",
    "#numero_feats = [len(set(x[1])) for x in Defs]\n",
    "#print numero_feats, len(numero_feats), max(numero_feats), min(numero_feats), sum(numero_feats)/len(numero_feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining *Intercorr_str_tax* and *Rank_PF*\n",
    "\n",
    "In this function we complement the features selected based on the *Intercorr_str_tax* variable with features selected based on frequency of mention, that is to say *Rank_PF*.\n",
    "The goal is for each concept to have at least **6** features. For those concepts where the *Intercorr_str_tax* variable does not provide with 6 features, we complement the list of features by selecting the rest based on Rank_PF (frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDefs_Intercorr_Rank (max_num_feats, names_list):\n",
    "    \"Given an xlsx file it returns all the concepts feature values as they appear in the original dataset\"\n",
    "    #Dataframe for excel document\n",
    "    df = pd.read_excel( pathh + 'CONCS_FEATS_concstats_brm.xlsx')\n",
    "    \n",
    "    #Create a list with all concept names\n",
    "    #names = df['Concept'].unique().tolist()\n",
    "    #names = map(str, names)\n",
    "    \n",
    "    Concepts = []\n",
    "    # Extract list of features for each concept\n",
    "    for name in names_list:\n",
    "        # Locating the concept by name\n",
    "        row = df.loc[df['Concept'] == name]\n",
    "        \n",
    "        # Reading features and Rank_PF values\n",
    "        selected_features = row[['Feature','Rank_PF','Intercorr_Str_Tax']].values.tolist()\n",
    "        # Setting strings into an appropiate format\n",
    "        selected_features = map(lambda x: [str(x[0]), int(x[1]), float(x[2])], selected_features) \n",
    "        \n",
    "        # Keeping only the features where Intercorr_Str_Tax is higher than 0... \n",
    "        selected_features1 = [x for x in selected_features if x[2] > 0]\n",
    "        \n",
    "        # Sorting by Intercorr_Str_Tax and keeping only the highest (the 'max_num_feats' highest)... \n",
    "        selected_features1 = sorted(selected_features1, key = lambda x: x[2])\n",
    "\n",
    "        # Are there at least 6 features??\n",
    "        if len(selected_features1) < 6:\n",
    "            # Se ordenan por Rank_PF...\n",
    "            selected_features2 = sorted(selected_features, key = lambda x: x[1])\n",
    "            # Appending additional features (based on Rank_PF)\n",
    "            for featt in selected_features2:\n",
    "                if featt not in selected_features1 and len(selected_features1) < 6:  # QUITAR ESTA SEGUNDA CONDICIÃ“N...\n",
    "                    selected_features1.append(featt)\n",
    "        \n",
    "        # Keeping only the feature's name (removing Rank_PF and Intercorr_Str_Tax value)\n",
    "        selected_features1 = [x[0] for x in selected_features1]\n",
    "\n",
    "        # Creating final representation \n",
    "        Concepts.append([str(name), Repeat_features(selected_features1[:max_num_feats])]) \n",
    "    return Concepts\n",
    "\n",
    "#Defs = ReadDefs_Intercorr_Rank(10)\n",
    "#print Defs\n",
    "\n",
    "# Luego de eliminar los features con valor intercorr = 0 quiero ver cual es el promedio de features por concepto, \n",
    "# asÃ­ como max y min\n",
    "#numero_feats = [len(set(x[1])) for x in Defs]\n",
    "#print numero_feats, len(numero_feats), max(numero_feats), min(numero_feats), sum(numero_feats)/len(numero_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing ID vectors into memory\n",
    "\n",
    "### Creating definitions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDictionary( mode , max_num_feats, names_list):\n",
    "    global Dict_defs\n",
    "    if mode == 'normal':\n",
    "        data = ReadDefinitions(max_num_feats, names_list)\n",
    "    elif mode == 'Rank_PF':\n",
    "        data = ReadDefs_RankPF(max_num_feats, names_list)  # Based on Rank_PF (most commonly mentioned features)\n",
    "    elif mode == 'Disting':\n",
    "        data = ReadDefs_Disting(max_num_feats, names_list)\n",
    "    elif mode == 'Intercorr_str':\n",
    "        data = ReadDefs_Intercorr_str(max_num_feats, names_list)\n",
    "    elif mode == 'Intercorr-Rank':\n",
    "        data = ReadDefs_Intercorr_Rank(max_num_feats, names_list)\n",
    "        \n",
    "    for concept in data:\n",
    "        Dict_defs[concept[0]] = TranslateFeats(concept[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list (L):\n",
    "    \"Recursive function that flats a list of lists (at any level)\"\n",
    "    if L == []:\n",
    "        return L\n",
    "    if type(L[0]) is list:\n",
    "        return flat_list(L[0]) + flat_list(L[1:])\n",
    "    return L[:1] + flat_list(L[1:])\n",
    "\n",
    "def FeatureVectors(Dic):\n",
    "    \"It extract from the definition dictionary all the feature type vectors ('is','has','color', etc...)\"\n",
    "    global feature_vectors\n",
    "    featt = []\n",
    "    vals = Dic.values()\n",
    "    for l in vals:\n",
    "        for p in l:\n",
    "            featt.append(p[0])\n",
    "    feature_vectors = list(set(featt))\n",
    "    \n",
    "    \n",
    "def SaveConcepts(Dic):\n",
    "    \"\"\"Given a definitions dictionary it stores in memory the entire set of concepts in the dictionary (including feature vectors)\"\"\"\n",
    "    keys = Dic.keys()\n",
    "    vals = Dic.values()\n",
    "    all_concepts = list(set(flat_list(vals) + keys))\n",
    "    # Process for storing list of concepts in memory\n",
    "    for concept in all_concepts:\n",
    "        HDvector(N,concept) #This creates an object and store it in memory    \n",
    "    \n",
    "def CreateSemanticPointer (PairList):\n",
    "    \"Turns list as [[feat1,feat_val],[feat2,feat_val],[feat3,feat_val]] into vector feat1*feat_val + feat2*feat_val ...\"\n",
    "    if len(PairList) == 0:\n",
    "        return HDvector(N)\n",
    "    vecs = []\n",
    "    for pair in PairList:\n",
    "        vecs.append(Dict[pair[0]] * Dict[pair[1]])\n",
    "    return ADD(vecs)\n",
    "\n",
    "def SaveDefinitions(Dic):\n",
    "    \"\"\"Given the definitions dictionary, and having all its concepts previously stored in memory, this functions\n",
    "       creates a definition vector (semantic pointer) using HD operations and assign it as a pointer to an \n",
    "       object vector (ID vector).\"\"\"\n",
    "    global feature_vectors\n",
    "    # Going through all elements in dictionary\n",
    "    for key, value in Dic.iteritems():\n",
    "        Dict[key].setPointer(CreateSemanticPointer(value))\n",
    "        \n",
    "def NormalizeHammDist (Dist_list):\n",
    "    \"Given a distance list of the form [['name', dist], ['name', dist], ... ], it normalize each distance and return a list with the same form\"\n",
    "    for i in range(len(Dist_list)):\n",
    "        Dist_list[i][1] = round( 1. - Dist_list[i][1] / float(N), 3 ) \n",
    "    return Dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_mem( mode = 'normal', max_num_feats = 100 , names_list = None):\n",
    "    init()\n",
    "    print \"Begining to encode dataset...\"\n",
    "    thr = 0.45 * N\n",
    "    # Read dataset and create definition dictionary\n",
    "    CreateDictionary( mode, max_num_feats, names_list )\n",
    "    # Feature vectors\n",
    "    FeatureVectors(Dict_defs)\n",
    "    # Save concepts into memory (ID vectors)\n",
    "    SaveConcepts(Dict_defs)\n",
    "    # Associate definitions to concepts into memory (SP vectors)\n",
    "    SaveDefinitions(Dict_defs)\n",
    "    print \"End of encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading similarity from distance matrix (McRae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def McRae_simi (pair_concepts):\n",
    "    \"Given a pair of concepts (in a list) it consults the similarity from the cos_matrix... file\"\n",
    "    try: \n",
    "        df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','1st_200')\n",
    "        return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','2nd_200')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]\n",
    "        except:\n",
    "            df = pd.read_excel(pathh + 'cos_matrix_brm_IFR.xlsx','last_141')\n",
    "            return list(df.loc[df['CONCEPT'] == pair_concepts[0]][pair_concepts[1]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity using NLTK library functions\n",
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def get_concepts_list ():\n",
    "    \"Returns a list of strings: the names of the concepts\"\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    return map(str, list(df['Concept']))\n",
    "    \n",
    "def get_synset (concept):\n",
    "    \"Given a concept name (string) it returns its synset (string)\"\n",
    "    # Dataframe for excel document\n",
    "    df = pd.read_excel(pathh + 'CONCS_Synset_brm.xlsx') #../McRaedataset/CONCS_Synset_brm.xlsx')\n",
    "    row = df.loc[df['Concept'] == concept]\n",
    "    return str(list(row['Synset'] )[0])\n",
    "\n",
    "def apply_sim_metric ( similarity_metric, num, in_concept, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    dist_list = []\n",
    "    for c in Concepts:\n",
    "        c_synset = wn.synset( get_synset(c) )\n",
    "        if corpus:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset, corpus), 3) ])\n",
    "        else:\n",
    "            dist_list.append([c, round(similarity_metric(in_concept, c_synset), 3) ])\n",
    "    return sorted(dist_list, key = lambda r : r[1], reverse = True ) [:num]\n",
    "\n",
    "def similarity_fun ( similarity_metric, pair, corpus = None):\n",
    "    \"Given a similarity_metric function it returns a list of the num closest concepts to 'concept'\"\n",
    "    c_synset_1 = wn.synset( get_synset(pair[0]))\n",
    "    c_synset_2 = wn.synset( get_synset(pair[1]))\n",
    "    if corpus:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2, corpus), 3)\n",
    "    else:\n",
    "        return round(similarity_metric(c_synset_1, c_synset_2), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
